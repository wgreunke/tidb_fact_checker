# -*- coding: utf-8 -*-
"""multi_source_doc_loader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y2RbcLANihvvgbJ6wBHyqcaH4L4Cg0O9

#This file takes in a list of topics and then pulls documents from the web that can be used for fact checking.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain_community
# %pip install sentence-transformers
# %pip install tidb-vector
# %pip install pymysql
# %pip install sqlalchemy
# %pip install mysqlclient
# %pip install langchain_text_splitters
# %pip install pypdf

"""#Sources of Data

#To Load

https://www.fppc.ca.gov/media/factsheets.html - CA Election Fact Sheets

https://www.fec.gov/introduction-campaign-finance/election-results-and-voting-information/ - Federal Election Results






https://www.dhs.gov/publications-library/collections/fact-sheets Homeland Security Fact Sheet

https://www.fsa.usda.gov/news-room/fact-sheets/index - Farm Service Agency

https://www.cdc.gov/ CDC - Go by the letters

#Finished Loading into Tidb
https://www.who.int/news-room/fact-sheets World Health Organization

https://www.whitehouse.gov/dpc/fact-sheets/ White House Fact Sheets

https://www.commerce.gov/news/fact-sheets Departement of Comerce Face Sheets


"""

#This takes a parent page and returns a list of links on that page which can then be loaded using the next cell


#Main page has a list of fact sheets.
#Create a list of the links that are on this page: https://www.who.int/news-room/fact-sheets
import requests
from bs4 import BeautifulSoup

#These urls ahve all been loaded by Ward
#parent_url="https://www.who.int/news-room/fact-sheets" #Done
#parent_url="https://www.whitehouse.gov/dpc/fact-sheets/" #Done
#parent_url="https://www.commerce.gov/news/fact-sheets?q=/news/fact-sheets&page=2"
#parent_url="https://www.commerce.gov/news/fact-sheets?q=/news/fact-sheets&page=3"
#parent_url="https://www.commerce.gov/news/fact-sheets?q=/news/fact-sheets&page=0"
#parent_url="https://www.commerce.gov/news/fact-sheets?q=/news/fact-sheets&page=4"
#parent_url="https://www.commerce.gov/news/fact-sheets?q=/news/fact-sheets&page=5"
parent_url="https://www.cdc.gov/heart-disease/site.html"



#This function takes a url and then returns the links that are listed on the page.
def extract_links(url):
  """Extracts links from a given URL.
  Args:url: The URL to extract links from.
  Returns:A list of extracted links.
  """
  response = requests.get(url)
  soup = BeautifulSoup(response.content, 'html.parser')
  links = []
  for link in soup.find_all('a', href=True):
    links.append(link['href'])
  return links

# Build the list of links
links = extract_links(parent_url)


#Print each link on a new line
for link in links:
  print(link)

"""#PDF Web Reader
https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/

Use Selenium to try and imitate a web browser.
"""

#https://stackoverflow.com/questions/77856952/running-a-script-in-google-colab-throws-back-no-results
import pandas as pd
from bs4 import BeautifulSoup
from tabulate import tabulate
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.headless = True
driver = webdriver.Chrome(options=options)

driver.get(parent_url)

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')


def extract_links(url):
  """Extracts links from a given URL.
  Args:url: The URL to extract links from.
  Returns:A list of extracted links.
  """
  response = requests.get(url)
  soup = BeautifulSoup(response.content, 'html.parser')
  links = []
  for link in soup.find_all('a', href=True):
    links.append(link['href'])
  return links

# Build the list of links
links = extract_links(parent_url)


#Print each link on a new line
for link in links:
  print(link)


driver.quit()

#PDF Web Reader

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("https://www.fsa.usda.gov/Assets/USDA-FSA-Public/usdafiles/FactSheets/2019/highly-fractionated-indian-land-loan-program-fact_sheet-aug_2019.pdf")
pages = loader.load_and_split()



"""# HTML Reader"""

#Clean up the list of links

#Remove any items that match this string exactly https://www.whitehouse.gov/briefing-room/statements-releases/
print(len(links))
#This removed some redunent links
#links = [link for link in links if link != 'https://www.whitehouse.gov/briefing-room/statements-releases/']


#For comerce.gov, only return the links that have form
#/news/fact-sheets/2022/04/building-better-america-commerce-department-fact-sheet-rural-communities"
#Only return the links that have the text "news/fact-sheets" contatined in the text
links = [link for link in links if "/heart-disease/" in link]


#Special case where child links are reletive and you need to add the parent page.  Need to take out if not relevant for a page.
print("Links before appending")
for link in links:
  print(link)

base_url="https://www.cdc.gov"
for i,link in enumerate(links):
  if not link.startswith(base_url):
    links[i]=base_url + link


print("Links after appending")
for link in links:
  print(link)

#Given the list of links, load each page using beautiful soup page
from langchain_community.document_loaders import BSHTMLLoader
import requests
from bs4 import BeautifulSoup
from langchain.schema import Document
#https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/html/

def load_from_url(url):
    """Loads HTML content from a URL and returns a Langchain document."""
    try:
      response = requests.get(url)
      soup = BeautifulSoup(response.content, 'html.parser')
      # Extract text content from the soup object (customize as needed)
      text_content = soup.get_text()
      print("The text content is")
      print(text_content)
      #catch the error if the url is bad
      #Return the result as a document object that is the same that LangChain uses
      return Document(page_content=text_content, metadata={"source": url})
    except:
      return []

#url="https://www.bbc.com/news/articles/c79w810e38no"

docs=[]
for link in links:
  docs.append(load_from_url(link))
#print(docs[0].page_content)
print(len(docs))
print(docs[1])

#Remove any items from the list that are empty
docs = [doc for doc in docs if doc]
#What is the lenght of the doc list
print("The length of the docs list is")
print(len(docs))

#Chunk the docs into smaller pieces before loading them into the database.
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # Set your desired chunk size
    chunk_overlap=200,  # Set your desired overlap
    add_start_index=True  # Optionally add start indices to chunks
)


#Take the list of docs and split them using the text splitter object
docs_chunked = text_splitter.split_documents(docs)

#What is the lenght of the doc list
print(len(docs_chunked))
print(docs_chunked[0].page_content)
print(docs_chunked[0].metadata)

print(len(docs_chunked))

#Now that the docs are loaded, turn them to vectors and put them into Tidb

import os
from tidb_vector.integrations import TiDBVectorClient
from sentence_transformers import SentenceTransformer

#Choose an embedding engine
embed_model=SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5")
embed_model_dims=embed_model.get_sentence_embedding_dimension()


#Connection String
tidb_connection_string="mysql+mysqldb://4DdAZ4FvfFtMb6Y.root:TlaxhK73yueEhQi9@gateway01.us-west-2.prod.aws.tidbcloud.com:4000/test?ssl_mode=VERIFY_IDENTITY&ssl_ca=/content/isrgrootx1.pem"



#Create the vector store object
vector_store=TiDBVectorClient(
  table_name="embedded_documents",
  connection_string=tidb_connection_string,
  vector_dimension=embed_model_dims,
  #This drops any data that was previously stored in the DB.
  #drop_existing_table=True,
)

#Function to embed text into a vector
def text_to_embedding(text):
  embedding=embed_model.encode(text)
  embedding=embedding.tolist()
  return embedding

#Test to make sure the chunking worked
print(docs_chunked[0].page_content)
print(docs_chunked[0].metadata)
print(text_to_embedding(docs_chunked[0].page_content))

#This does the actual insert into the tidb database


#Do the insert.
#Takes in a docs list.

#vector_store.insert(
#    texts=["Hello World"],  #This is raw text
#    embeddings=[temp],  #This is the embedding that uses the cotnet
#    metadatas=["Test Metadata"])  #Meta data is plan text

#https://docs.pingcap.com/tidbcloud/vector-search-get-started-using-python

#This inserts a list of docs.
#Warning  - note the [] around doc.  This takes the list and turns it into a list.  Was the cause of my problmes.

"""
vector_store.insert(
    texts= [doc.page_content for doc in docs_chunked],
    embeddings=[text_to_embedding(doc.page_content) for doc in docs_chunked],
    metadatas=[doc.metadata for doc in docs_chunked],
)
"""


#Create a for loop that loops through the docs_chunked and does an insert
for doc in docs_chunked:
  vector_store.insert(
    texts=[doc.page_content],
    embeddings=[text_to_embedding(doc.page_content)],
    metadatas=[doc.metadata],
  )
print("Finished loading into tidb")